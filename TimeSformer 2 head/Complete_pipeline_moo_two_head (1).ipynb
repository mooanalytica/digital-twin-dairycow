{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI-0hnjXBrxM",
        "outputId": "702e41e3-d143-477b-dc88-0a1704d67aae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga8omaQ6AFiF",
        "outputId": "62f9f85b-034e-47f8-d96e-6a09f8240340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.199-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.17-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.199-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.17-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.199 ultralytics-thop-2.0.17\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q uninstall -y peft bitsandbytes albumentations albucore opencv-python opencv-python-headless || true\n",
        "\n",
        "\n",
        "!pip -q install \"transformers==4.41.2\" \"decord==0.6.0\" \"torchmetrics==1.4.0\" \"scikit-learn==1.5.1\" \\\n",
        "                albumentations==1.4.8 albucore==0.0.14 opencv-python-headless==4.10.0.84 --no-cache-dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21LvlkQoCKVu",
        "outputId": "e249ef31-1919-4c0e-fc51-84bf4e06cd31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m185.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m338.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m129.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m315.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m234.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIG\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Input / Output\n",
        "VIDEO_PATH           = \"/content/cows.mp4\"\n",
        "OUT_DIR              = Path(\"/content/cow_logs\")\n",
        "WRITE_ANNOTATED_MP4  = True\n",
        "\n",
        "# YOLO / ByteTrack\n",
        "YOLO_WEIGHTS         = \"/content/best.pt\"\n",
        "YOLO_IMGSZ           = 960\n",
        "YOLO_CONF            = 0.60\n",
        "YOLO_IOU             = 0.50\n",
        "TRACKER_YAML         = \"bytetrack.yaml\"\n",
        "\n",
        "# Two head TimeSformer shared backbone + 2 heads classifiers\n",
        "MULTI_MODEL_DIR      = \"/content/drive/MyDrive/Models/timesformer-cows-multitask/backbone_hf\"\n",
        "CLF_RES              = 224               # 112 for speed\n",
        "WINDOW_FRAMES        = 12                # 12 frames/clip\n",
        "SAMPLE_FPS           = 1.6               # 16 frames per 10 s\n",
        "WINDOW_OVERLAP       = 0.5               # 0.5, decision every 5 s at 1.6 fps\n",
        "SMOOTH_K             = 3                 # majority over last K labels\n",
        "MIN_WARMUP_FRAMES    = 8\n",
        "\n",
        "# Segmentation / tracking timeouts\n",
        "INACTIVITY_TIMEOUT_S = 10\n",
        "MIN_SEGMENT_S        = 2\n",
        "\n",
        "# Final joint 7-class space\n",
        "CLASSES = [\n",
        "    \"Standing\",\n",
        "    \"Lying\",\n",
        "    \"Drinking\",\n",
        "    \"Feeding & Standing\",\n",
        "    \"Feeding & Lying\",\n",
        "    \"Ruminating & Standing\",\n",
        "    \"Ruminating & Lying\",\n",
        "]\n",
        "LABEL2ID = {c:i for i,c in enumerate(CLASSES)}\n",
        "ID2LABEL = {i:c for c,i in LABEL2ID.items()}\n",
        "\n",
        "# Heads internal label spaces\n",
        "POSTURE  = {\"Standing\": 0, \"Lying\": 1}\n",
        "ACTIVITY = {\"None\": 0, \"Drinking\": 1, \"Feeding\": 2, \"Ruminating\": 3}\n",
        "\n",
        "# Valid (posture, activity) combinations, final 7-class index\n",
        "PAIR_LIST = [\n",
        "    (\"Standing\", \"None\",        LABEL2ID[\"Standing\"]),\n",
        "    (\"Lying\",    \"None\",        LABEL2ID[\"Lying\"]),\n",
        "    (\"Standing\", \"Drinking\",    LABEL2ID[\"Drinking\"]),\n",
        "    (\"Standing\", \"Feeding\",     LABEL2ID[\"Feeding & Standing\"]),\n",
        "    (\"Lying\",    \"Feeding\",     LABEL2ID[\"Feeding & Lying\"]),\n",
        "    (\"Standing\", \"Ruminating\",  LABEL2ID[\"Ruminating & Standing\"]),\n",
        "    (\"Lying\",    \"Ruminating\",  LABEL2ID[\"Ruminating & Lying\"]),\n",
        "]\n",
        "\n",
        "PAD_TARGET_HW = (640, 640)\n",
        "\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(\"Configured (two-head).\")\n"
      ],
      "metadata": {
        "id": "IR0UGFoNCXbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78ea428b-f00e-4840-9ead-267b2816a1c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configured (two-head).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Imports & helpers\n",
        "\n",
        "import os, time, math, json, gc\n",
        "import cv2, torch, numpy as np, pandas as pd\n",
        "from collections import defaultdict, deque\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from transformers import AutoConfig, TimesformerModel\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "def sec_to_hms(t):\n",
        "    h = int(t//3600); m = int((t%3600)//60); s = int(t%60)\n",
        "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
        "\n",
        "# Scale to fit center padding\n",
        "def center_pad_bbox_crop(img_rgb, xyxy, pad_target=(640, 640), out_size=224):\n",
        "    \"\"\"\n",
        "    Crop YOLO bbox, if larger than pad canvas, scale down to fit, center-pad to pad_target\n",
        "    ,resize to out_size. Returns RGB uint8 or None if bbox empty.\n",
        "    \"\"\"\n",
        "    H, W = img_rgb.shape[:2]\n",
        "    x1, y1, x2, y2 = map(int, xyxy)\n",
        "    x1 = max(0, x1); y1 = max(0, y1); x2 = min(W-1, x2); y2 = min(H-1, y2)\n",
        "    crop = img_rgb[y1:y2, x1:x2]\n",
        "    if crop.size == 0:\n",
        "        return None\n",
        "\n",
        "    th, tw = pad_target\n",
        "    h, w = crop.shape[:2]\n",
        "    if h > th or w > tw:\n",
        "        scale = min(th / h, tw / w)\n",
        "        nh, nw = max(1, int(round(h * scale))), max(1, int(round(w * scale)))\n",
        "        crop = cv2.resize(crop, (nw, nh), interpolation=cv2.INTER_LINEAR)\n",
        "        h, w = nh, nw\n",
        "\n",
        "    top = (th - h) // 2; bottom = th - h - top\n",
        "    left = (tw - w) // 2; right = tw - w - left\n",
        "    padded = cv2.copyMakeBorder(crop, top, bottom, left, right,\n",
        "                                borderType=cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
        "\n",
        "    if out_size and (out_size != th):\n",
        "        padded = cv2.resize(padded, (out_size, out_size), interpolation=cv2.INTER_LINEAR)\n",
        "    return padded\n",
        "\n",
        "# ImageNet norm\n",
        "IMNET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
        "IMNET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
        "def preprocess_clip(frames_hw3):  # list of T RGB (H,W,3) uint8\n",
        "    arr = np.stack(frames_hw3, axis=0).astype(np.float32) / 255.0\n",
        "    arr = (arr - IMNET_MEAN) / IMNET_STD\n",
        "    arr = arr.transpose(0,3,1,2)  # T,C,H,W\n",
        "    return torch.from_numpy(arr).unsqueeze(0)  # 1,T,C,H,W\n",
        "\n",
        "def majority(lst):\n",
        "    if not lst: return None\n",
        "    return max(set(lst), key=lst.count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0mPtNyyD-eN",
        "outputId": "717ab6f7-2fd2-4c60-de69-fb344a1fb0d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Two-head TimeSformer backbone + heads + joint decode\n",
        "\n",
        "class MultiTaskTimeSformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Loads a Timesformer backbone from HF ckpt and adds two linear heads:\n",
        "      - posture (Standing/Lying), 2 classes\n",
        "      - activity (None/Drinking/Feeding/Ruminating), 4 classes\n",
        "    \"\"\"\n",
        "    def __init__(self, ckpt_dir: str):\n",
        "        super().__init__()\n",
        "        cfg = AutoConfig.from_pretrained(ckpt_dir)\n",
        "        self.backbone = TimesformerModel.from_pretrained(ckpt_dir, config=cfg)\n",
        "        hidden = cfg.hidden_size\n",
        "        p_drop = getattr(cfg, \"hidden_dropout_prob\", getattr(cfg, \"dropout\", 0.0))\n",
        "        self.dropout = nn.Dropout(p_drop)\n",
        "        self.posture_head  = nn.Linear(hidden, 2)\n",
        "        self.activity_head = nn.Linear(hidden, 4)\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        # pixel_values: [B, T, C, H, W]  (as constructed in preprocess_clip)\n",
        "        out = self.backbone(pixel_values=pixel_values)       # Base model forward\n",
        "        cls = out.last_hidden_state[:, 0]                    # CLS token\n",
        "        x = self.dropout(cls)\n",
        "        return self.posture_head(x), self.activity_head(x)   # logits_p, logits_a\n",
        "\n",
        "@torch.no_grad()\n",
        "def joint_decode(posture_logits, activity_logits):\n",
        "    \"\"\"\n",
        "    Combine posture+activity heads into the 7 class label space via masked log sum.\n",
        "    Returns: pred_idx [B], joint_log_scores [B, 7]\n",
        "    \"\"\"\n",
        "    lp = F.log_softmax(posture_logits,  dim=-1)   # [B,2]\n",
        "    la = F.log_softmax(activity_logits, dim=-1)   # [B,4]\n",
        "    B = lp.size(0)\n",
        "    joint = torch.full((B, len(CLASSES)), -1e9, device=lp.device)\n",
        "\n",
        "    for p_str, a_str, cls_idx in PAIR_LIST:\n",
        "        p = POSTURE[p_str]; a = ACTIVITY[a_str]\n",
        "        joint[:, cls_idx] = lp[:, p] + la[:, a]   # log-space sum == add\n",
        "\n",
        "    preds = joint.argmax(-1)\n",
        "    return preds, joint\n",
        "\n",
        "# Instantiate two-head model\n",
        "mt_cfg = AutoConfig.from_pretrained(MULTI_MODEL_DIR)  # for info/consistency\n",
        "clf_twohead = MultiTaskTimeSformer(MULTI_MODEL_DIR).to(device).eval()\n",
        "print(\"Two-head TimeSformer loaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CosC6RRUEDDd",
        "outputId": "274703b6-614a-499f-cd7e-83b03ddd8666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Two-head TimeSformer loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLO detector + ByteTrack\n",
        "\n",
        "yolo = YOLO(YOLO_WEIGHTS)\n",
        "try:\n",
        "    if device == \"cuda\":\n",
        "        #try: yolo.model.half()\n",
        "        pass\n",
        "    yolo.fuse()\n",
        "except Exception as e:\n",
        "    print(\"YOLO optimize hint:\", e)\n",
        "print(\"YOLO loaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iebPE_tDLEHe",
        "outputId": "7bfccb67-cbbc-4899-8785-bd50bd5529e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLO11s summary (fused): 100 layers, 9,413,187 parameters, 0 gradients, 21.3 GFLOPs\n",
            "YOLO loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main pipeline\n",
        "\n",
        "# Video props\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "assert cap.isOpened(), f\"Cannot open video: {VIDEO_PATH}\"\n",
        "video_fps   = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
        "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "video_w     = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "video_h     = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "cap.release()\n",
        "\n",
        "# Sampling & windowing\n",
        "sample_stride   = max(1, int(round(video_fps / max(0.1, float(SAMPLE_FPS)))))\n",
        "step_frames     = max(1, int(round(WINDOW_FRAMES * (1.0 - float(WINDOW_OVERLAP)))))  # in SAMPLES\n",
        "timeout_frames  = int(round(INACTIVITY_TIMEOUT_S * video_fps))\n",
        "\n",
        "print(f\"fps={video_fps:.2f} frames={frame_count} size=({video_w}x{video_h})\")\n",
        "print(f\"Sample every {sample_stride} frames (~{SAMPLE_FPS} fps); window={WINDOW_FRAMES}, step={step_frames}, clf_res={CLF_RES}\")\n",
        "\n",
        "# Per-track state\n",
        "buffers              = defaultdict(lambda: deque(maxlen=WINDOW_FRAMES))  # cow_id -> deque of padded crops\n",
        "last_sample_frame    = defaultdict(lambda: -10**9)                        # cow_id -> last raw frame idx sampled\n",
        "last_classify_frame  = defaultdict(lambda: -10**9)                        # cow_id -> last raw frame idx classified\n",
        "last_seen_frame      = dict()                                            # cow_id -> last frame seen\n",
        "pred_hist_joint      = defaultdict(lambda: deque(maxlen=SMOOTH_K))       # cow_id -> last K *final* labels (strings)\n",
        "\n",
        "active_event = dict()  # cow_id -> {\"label\": str, \"start_frame\": int}\n",
        "events       = []      # finalized segments\n",
        "\n",
        "# Annotated video writer\n",
        "writer = None\n",
        "if WRITE_ANNOTATED_MP4:\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    writer = cv2.VideoWriter(str(OUT_DIR / \"annotated.mp4\"), fourcc, video_fps, (video_w, video_h))\n",
        "\n",
        "start_time = time.time()\n",
        "frame_idx  = -1\n",
        "\n",
        "# ByteTrack stream\n",
        "gen = yolo.track(\n",
        "    source=VIDEO_PATH, stream=True, imgsz=YOLO_IMGSZ,\n",
        "    conf=YOLO_CONF, iou=YOLO_IOU, tracker=TRACKER_YAML,\n",
        "    device=0 if device==\"cuda\" else 'cpu', verbose=False, persist=True\n",
        ")\n",
        "\n",
        "# Ensure fast tracker dependency is present\n",
        "try:\n",
        "    import lap\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "DEBUG_DUMPS = 0  # set to 0 to save a few clip grids\n",
        "_dbg_dumped = 0\n",
        "\n",
        "for res in tqdm(gen, desc=\"Processing\"):\n",
        "    frame_idx += 1\n",
        "    img_bgr = res.orig_img\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    xyxy = None; ids_arr = None\n",
        "    if res.boxes is not None and res.boxes.xyxy is not None:\n",
        "        xyxy = res.boxes.xyxy.cpu().numpy()\n",
        "        ids  = res.boxes.id\n",
        "        ids_arr = ids.cpu().numpy().astype(int) if ids is not None else np.arange(len(xyxy), dtype=int)\n",
        "\n",
        "        for bb, tid in zip(xyxy, ids_arr):\n",
        "            last_seen_frame[tid] = frame_idx\n",
        "\n",
        "            # sample into clip buffer at SAMPLE_FPS\n",
        "            if frame_idx - last_sample_frame[tid] >= sample_stride:\n",
        "                crop = center_pad_bbox_crop(img_rgb, bb, pad_target=PAD_TARGET_HW, out_size=CLF_RES)\n",
        "                if crop is not None:\n",
        "                    buffers[tid].append(crop)\n",
        "                    last_sample_frame[tid] = frame_idx\n",
        "\n",
        "            # readiness, spaced by SAMPLES since last classify\n",
        "            n_samples = len(buffers[tid])\n",
        "            ready_full = (n_samples >= WINDOW_FRAMES)\n",
        "            ready_warm = (active_event.get(tid) is None) and (n_samples >= MIN_WARMUP_FRAMES)\n",
        "            step_ok    = (frame_idx - last_classify_frame[tid]) >= (sample_stride * step_frames)\n",
        "\n",
        "            if (ready_full or ready_warm) and step_ok:\n",
        "                clip = list(buffers[tid])\n",
        "                if n_samples < WINDOW_FRAMES:\n",
        "                    clip = clip + [clip[-1]] * (WINDOW_FRAMES - n_samples)  # pad to T\n",
        "\n",
        "                pixel_values = preprocess_clip(clip).to(device)\n",
        "                with torch.no_grad():\n",
        "                    logits_p, logits_a = clf_twohead(pixel_values=pixel_values)   # two heads\n",
        "                    pred_ids, joint = joint_decode(logits_p, logits_a)            # [B], [B,7]\n",
        "                    pred_idx = int(pred_ids.item())\n",
        "                    label    = ID2LABEL[pred_idx]\n",
        "\n",
        "                pred_hist_joint[tid].append(label)\n",
        "                smooth_label = majority(list(pred_hist_joint[tid]))\n",
        "                last_classify_frame[tid] = frame_idx\n",
        "\n",
        "                # Debug dump of exactly what the model saw\n",
        "                if _dbg_dumped < DEBUG_DUMPS:\n",
        "                    T = len(clip); cols=4; rows=math.ceil(T/cols)\n",
        "                    padf = np.zeros_like(clip[0], dtype=np.uint8)\n",
        "                    tiles = clip + [padf]*(rows*cols - T)\n",
        "                    grid = np.concatenate([np.concatenate(tiles[r*cols:(r+1)*cols], 1) for r in range(rows)], 0)\n",
        "                    out_path = OUT_DIR / f\"dbg_cow{tid}_f{frame_idx}_raw-{label.replace(' ','_')}_sm-{smooth_label.replace(' ','_')}.jpg\"\n",
        "                    cv2.imwrite(str(out_path), cv2.cvtColor(grid, cv2.COLOR_RGB2BGR))\n",
        "                    _dbg_dumped += 1\n",
        "\n",
        "                # Segment around window center\n",
        "                window_center_frame = frame_idx - (WINDOW_FRAMES // 2)\n",
        "                if tid not in active_event:\n",
        "                    active_event[tid] = {\"label\": smooth_label, \"start_frame\": max(0, window_center_frame)}\n",
        "                else:\n",
        "                    if smooth_label != active_event[tid][\"label\"]:\n",
        "                        st = active_event[tid][\"start_frame\"]\n",
        "                        et = max(st, window_center_frame)\n",
        "                        if (et - st) / video_fps >= MIN_SEGMENT_S:\n",
        "                            events.append({\n",
        "                                \"cow_id\": int(tid),\n",
        "                                \"activity\": active_event[tid][\"label\"],\n",
        "                                \"start_frame\": st,\n",
        "                                \"end_frame\": et\n",
        "                            })\n",
        "                        active_event[tid] = {\"label\": smooth_label, \"start_frame\": window_center_frame}\n",
        "\n",
        "    # close stale tracks\n",
        "    to_close = []\n",
        "    for tid, lastf in list(last_seen_frame.items()):\n",
        "        if frame_idx - lastf >= timeout_frames:\n",
        "            to_close.append(tid)\n",
        "    for tid in to_close:\n",
        "        if tid in active_event:\n",
        "            st = active_event[tid][\"start_frame\"]\n",
        "            et = last_seen_frame[tid]\n",
        "            if et < st: et = st\n",
        "            if (et - st) / video_fps >= MIN_SEGMENT_S:\n",
        "                events.append({\n",
        "                    \"cow_id\": int(tid),\n",
        "                    \"activity\": active_event[tid][\"label\"],\n",
        "                    \"start_frame\": st,\n",
        "                    \"end_frame\": et\n",
        "                })\n",
        "        buffers.pop(tid, None)\n",
        "        pred_hist_joint.pop(tid, None)\n",
        "        active_event.pop(tid, None)\n",
        "        last_seen_frame.pop(tid, None)\n",
        "\n",
        "    # overlay\n",
        "    if WRITE_ANNOTATED_MP4 and (xyxy is not None):\n",
        "        for bb, tid in zip(xyxy, ids_arr):\n",
        "            lab = active_event.get(int(tid), {}).get(\"label\")\n",
        "            if lab is None:\n",
        "                lab = \"estimating…\"\n",
        "            x1,y1,x2,y2 = map(int, bb)\n",
        "            cv2.rectangle(img_bgr, (x1,y1), (x2,y2), (0,255,0), 2)\n",
        "            cv2.putText(img_bgr, f\"{int(tid)}: {lab}\", (x1, max(0,y1-6)),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2, cv2.LINE_AA)\n",
        "        writer.write(img_bgr)\n",
        "\n",
        "# flush at end\n",
        "for tid, ev in list(active_event.items()):\n",
        "    st = ev[\"start_frame\"]\n",
        "    et = frame_idx\n",
        "    if et < st: et = st\n",
        "    if (et - st) / video_fps >= MIN_SEGMENT_S:\n",
        "        events.append({\n",
        "            \"cow_id\": int(tid),\n",
        "            \"activity\": ev[\"label\"],\n",
        "            \"start_frame\": st,\n",
        "            \"end_frame\": et\n",
        "        })\n",
        "if writer is not None:\n",
        "    writer.release()\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"Processed {frame_idx+1} frames in {elapsed:.1f}s \"\n",
        "      f\"(~{(frame_idx+1)/max(1.0,elapsed):.1f} FPS incl. det+track+class).\")\n",
        "if CLF_RES == 112:\n",
        "    print(\"Note: 112×112 mode chosen (faster, slight accuracy drop).\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wf-zRIhyL7EH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67e7f615-a205-4b83-bf2c-7699e85f3d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps=30.00 frames=5399 size=(1920x1080)\n",
            "Sample every 19 frames (~1.6 fps); window=12, step=6, clf_res=224\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lap>=0.5.12'] not found, attempting AutoUpdate...\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 0.7s\n",
            "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 5399it [03:58, 22.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 5399 frames in 239.5s (~22.5 FPS incl. det+track+class).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logs (CSV) + summary\n",
        "\n",
        "def save_and_summarize(events, out_dir: Path, fps: float):\n",
        "    if not events:\n",
        "        print(\"No segments produced — check model paths / labels / thresholds.\")\n",
        "        return None, None\n",
        "\n",
        "    df = pd.DataFrame(events)\n",
        "    df[\"start_sec\"]   = df[\"start_frame\"] / fps\n",
        "    df[\"end_sec\"]     = df[\"end_frame\"] / fps\n",
        "    df[\"duration_s\"]  = df[\"end_sec\"] - df[\"start_sec\"]\n",
        "    df[\"start_hms\"]   = df[\"start_sec\"].map(sec_to_hms)\n",
        "    df[\"end_hms\"]     = df[\"end_sec\"].map(sec_to_hms)\n",
        "    df[\"duration_min\"]= df[\"duration_s\"] / 60.0\n",
        "\n",
        "    csv_events = out_dir / \"cow_activity_events.csv\"\n",
        "    df.sort_values([\"cow_id\",\"start_frame\"]).to_csv(csv_events, index=False)\n",
        "    print(f\"Saved event log: {csv_events}\")\n",
        "\n",
        "    agg = (df.groupby([\"cow_id\",\"activity\"])[\"duration_s\"].sum().reset_index())\n",
        "    agg[\"duration_min\"] = agg[\"duration_s\"] / 60.0\n",
        "    csv_totals = out_dir / \"cow_activity_totals.csv\"\n",
        "    agg.sort_values([\"cow_id\",\"duration_s\"], ascending=[True, False]).to_csv(csv_totals, index=False)\n",
        "    print(f\"Saved totals:    {csv_totals}\")\n",
        "\n",
        "    print(\"\\nSample summary:\")\n",
        "    view = (agg.sort_values([\"cow_id\",\"duration_s\"], ascending=[True, False])\n",
        "              .groupby(\"cow_id\").head(5))\n",
        "    for _, row in view.iterrows():\n",
        "        print(f\"cow {int(row['cow_id'])}: {row['activity']} for {row['duration_min']:.1f} min\")\n",
        "    return csv_events, csv_totals\n",
        "\n",
        "csv_events, csv_totals = save_and_summarize(events, OUT_DIR, video_fps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqL_sHyu14GF",
        "outputId": "82c77096-8731-492b-c4e6-d9e9ac72451b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved event log: /content/cow_logs/cow_activity_events.csv\n",
            "Saved totals:    /content/cow_logs/cow_activity_totals.csv\n",
            "\n",
            "Sample summary:\n",
            "cow 1: Ruminating & Standing for 2.5 min\n",
            "cow 1: Ruminating & Lying for 0.4 min\n",
            "cow 2: Drinking for 2.5 min\n",
            "cow 2: Feeding & Lying for 0.4 min\n",
            "cow 3: Drinking for 1.2 min\n",
            "cow 3: Ruminating & Standing for 1.1 min\n",
            "cow 3: Ruminating & Lying for 0.6 min\n",
            "cow 4: Drinking for 0.8 min\n",
            "cow 4: Feeding & Standing for 0.4 min\n",
            "cow 4: Ruminating & Standing for 0.3 min\n",
            "cow 4: Ruminating & Lying for 0.1 min\n",
            "cow 5: Feeding & Standing for 0.4 min\n",
            "cow 5: Ruminating & Lying for 0.2 min\n",
            "cow 5: Ruminating & Standing for 0.1 min\n",
            "cow 6: Ruminating & Lying for 1.5 min\n",
            "cow 6: Drinking for 0.8 min\n",
            "cow 6: Feeding & Lying for 0.6 min\n",
            "cow 6: Ruminating & Standing for 0.1 min\n",
            "cow 7: Drinking for 2.4 min\n",
            "cow 7: Ruminating & Lying for 0.3 min\n",
            "cow 7: Feeding & Lying for 0.2 min\n",
            "cow 8: Drinking for 1.8 min\n",
            "cow 8: Feeding & Lying for 0.8 min\n",
            "cow 8: Ruminating & Standing for 0.2 min\n",
            "cow 8: Ruminating & Lying for 0.1 min\n",
            "cow 8: Feeding & Standing for 0.1 min\n",
            "cow 10: Feeding & Standing for 0.3 min\n",
            "cow 10: Ruminating & Lying for 0.2 min\n",
            "cow 10: Ruminating & Standing for 0.1 min\n",
            "cow 11: Drinking for 1.4 min\n"
          ]
        }
      ]
    }
  ]
}