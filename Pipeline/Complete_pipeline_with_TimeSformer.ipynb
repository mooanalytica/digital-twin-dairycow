{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI-0hnjXBrxM",
        "outputId": "7ee074a2-2cb1-4bc4-dec5-e6d4fac6c234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga8omaQ6AFiF",
        "outputId": "0925a231-ee75-4bfa-a639-2b6436cfe435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.199)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Collecting opencv-python>=4.6.0 (from ultralytics)\n",
            "  Using cached opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.17)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Using cached opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
            "Installing collected packages: opencv-python\n",
            "Successfully installed opencv-python-4.12.0.88\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q uninstall -y peft bitsandbytes albumentations albucore opencv-python opencv-python-headless || true\n",
        "\n",
        "\n",
        "!pip -q install \"transformers==4.41.2\" \"decord==0.6.0\" \"torchmetrics==1.4.0\" \"scikit-learn==1.5.1\" \\\n",
        "                albumentations==1.4.8 albucore==0.0.14 opencv-python-headless==4.10.0.84 --no-cache-dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21LvlkQoCKVu",
        "outputId": "7e20c33c-c6b1-4b94-da9f-38be828c70de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping peft as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m195.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIG\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Input / Output\n",
        "VIDEO_PATH           = \"/content/cows.mp4\"\n",
        "OUT_DIR              = Path(\"/content/cow_logs1\")\n",
        "WRITE_ANNOTATED_MP4  = True\n",
        "\n",
        "# YOLO / ByteTrack\n",
        "YOLO_WEIGHTS         = \"/content/best.pt\"\n",
        "YOLO_IMGSZ           = 960\n",
        "YOLO_CONF            = 0.60\n",
        "YOLO_IOU             = 0.50\n",
        "TRACKER_YAML         = \"bytetrack.yaml\"\n",
        "\n",
        "# TimeSformer\n",
        "SINGLE_MODEL_DIR     = \"/content/drive/MyDrive/Models/timesformer-cows2\"\n",
        "CLF_RES              = 224               # 112 for speed\n",
        "WINDOW_FRAMES        = 12                # T\n",
        "SAMPLE_FPS           = 1.6               # 16 frames per 10 s\n",
        "WINDOW_OVERLAP       = 0.5               # 0.5 = decision every 5 s at 1.6 fps\n",
        "SMOOTH_K             = 3                 # majority over last K labels\n",
        "MIN_WARMUP_FRAMES    = 8\n",
        "\n",
        "# Segmentation / tracking timeouts\n",
        "INACTIVITY_TIMEOUT_S = 10\n",
        "MIN_SEGMENT_S        = 2\n",
        "\n",
        "# Class map\n",
        "SINGLE_ID2LABEL = {\n",
        "    0: \"Standing\",\n",
        "    1: \"Lying\",\n",
        "    2: \"Drinking\",\n",
        "    3: \"Feeding & Standing\",\n",
        "    4: \"Feeding & Lying\",\n",
        "    5: \"Ruminating & Standing\",\n",
        "    6: \"Ruminating & Lying\",\n",
        "}\n",
        "\n",
        "PAD_TARGET_HW = (640, 640)\n",
        "\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(\"Configured.\")"
      ],
      "metadata": {
        "id": "IR0UGFoNCXbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34be4b09-3e82-46d4-d402-0fa3750785bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports & helpers\n",
        "\n",
        "import os, time, math, json, gc\n",
        "import cv2, torch, numpy as np, pandas as pd\n",
        "from collections import defaultdict, deque\n",
        "from tqdm import tqdm\n",
        "from ultralytics import YOLO\n",
        "from transformers import TimesformerForVideoClassification, AutoConfig\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "def sec_to_hms(t):\n",
        "    h = int(t//3600); m = int((t%3600)//60); s = int(t%60)\n",
        "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
        "\n",
        "# Scale to fit center padding\n",
        "def center_pad_bbox_crop(img_rgb, xyxy, pad_target=(640, 640), out_size=224):\n",
        "    \"\"\"\n",
        "    Crop YOLO bbox, if larger than pad canvas, scale down to fit, center-pad to pad_target\n",
        "    ,resize to out_size. Returns RGB uint8 or None if bbox empty.\n",
        "    \"\"\"\n",
        "    H, W = img_rgb.shape[:2]\n",
        "    x1, y1, x2, y2 = map(int, xyxy)\n",
        "    x1 = max(0, x1); y1 = max(0, y1); x2 = min(W-1, x2); y2 = min(H-1, y2)\n",
        "    crop = img_rgb[y1:y2, x1:x2]\n",
        "    if crop.size == 0:\n",
        "        return None\n",
        "\n",
        "    th, tw = pad_target\n",
        "    h, w = crop.shape[:2]\n",
        "    if h > th or w > tw:\n",
        "        scale = min(th / h, tw / w)\n",
        "        nh, nw = max(1, int(round(h * scale))), max(1, int(round(w * scale)))\n",
        "        crop = cv2.resize(crop, (nw, nh), interpolation=cv2.INTER_LINEAR)\n",
        "        h, w = nh, nw\n",
        "\n",
        "    top = (th - h) // 2; bottom = th - h - top\n",
        "    left = (tw - w) // 2; right = tw - w - left\n",
        "    padded = cv2.copyMakeBorder(crop, top, bottom, left, right,\n",
        "                                borderType=cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
        "\n",
        "    if out_size and (out_size != th):\n",
        "        padded = cv2.resize(padded, (out_size, out_size), interpolation=cv2.INTER_LINEAR)\n",
        "    return padded\n",
        "\n",
        "# ImageNet norm\n",
        "IMNET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
        "IMNET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
        "def preprocess_clip(frames_hw3):  # list of T RGB (H,W,3) uint8\n",
        "    arr = np.stack(frames_hw3, axis=0).astype(np.float32) / 255.0\n",
        "    arr = (arr - IMNET_MEAN) / IMNET_STD\n",
        "    arr = arr.transpose(0,3,1,2)  # T,C,H,W\n",
        "    return torch.from_numpy(arr).unsqueeze(0)  # 1,T,C,H,W\n",
        "\n",
        "def majority(lst):\n",
        "    if not lst: return None\n",
        "    return max(set(lst), key=lst.count)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0mPtNyyD-eN",
        "outputId": "b9db1cbc-2fed-4a64-dc63-18adcb5bef29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load models\n",
        "\n",
        "yolo = YOLO(YOLO_WEIGHTS)\n",
        "try:\n",
        "    if device == \"cuda\":\n",
        "        #try: yolo.model.half()\n",
        "        pass\n",
        "    yolo.fuse()\n",
        "except Exception as e:\n",
        "    print(\"YOLO optimize hint:\", e)\n",
        "\n",
        "cfg_single = AutoConfig.from_pretrained(SINGLE_MODEL_DIR)\n",
        "try:\n",
        "    if getattr(cfg_single, \"id2label\", None):\n",
        "        SINGLE_ID2LABEL = {int(k): v for k, v in cfg_single.id2label.items()}\n",
        "        print(\"Loaded id2label from checkpoint:\", SINGLE_ID2LABEL)\n",
        "    else:\n",
        "        print(\"No id2label in config; using SINGLE_ID2LABEL from CONFIG.\")\n",
        "except Exception as e:\n",
        "    print(\"id2label read error; using CONFIG map. Detail:\", e)\n",
        "\n",
        "clf_single = TimesformerForVideoClassification.from_pretrained(\n",
        "    SINGLE_MODEL_DIR, config=cfg_single, ignore_mismatched_sizes=True\n",
        ").to(device).eval()\n",
        "\n",
        "print(\"Models loaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CosC6RRUEDDd",
        "outputId": "02dc062d-c04d-4f67-c8cd-53147bd44557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLO11s summary (fused): 100 layers, 9,413,187 parameters, 0 gradients, 21.3 GFLOPs\n",
            "Loaded id2label from checkpoint: {0: 'Drinking', 1: 'Feeding & Lying', 2: 'Feeding & Standing', 3: 'Lying', 4: 'Ruminating & Lying', 5: 'Ruminating & Standing', 6: 'Standing'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Models/timesformer-cows2 and are newly initialized because the shapes did not match:\n",
            "- timesformer.embeddings.time_embeddings: found shape torch.Size([1, 8, 768]) in the checkpoint and torch.Size([1, 12, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main pipeline\n",
        "\n",
        "\n",
        "# Video props\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "assert cap.isOpened(), f\"Cannot open video: {VIDEO_PATH}\"\n",
        "video_fps   = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
        "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "video_w     = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "video_h     = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "cap.release()\n",
        "\n",
        "# Sampling & windowing\n",
        "sample_stride   = max(1, int(round(video_fps / max(0.1, float(SAMPLE_FPS)))))\n",
        "step_frames     = max(1, int(round(WINDOW_FRAMES * (1.0 - float(WINDOW_OVERLAP)))))  # in SAMPLES\n",
        "timeout_frames  = int(round(INACTIVITY_TIMEOUT_S * video_fps))\n",
        "\n",
        "print(f\"fps={video_fps:.2f} frames={frame_count} size=({video_w}x{video_h})\")\n",
        "print(f\"Sample every {sample_stride} frames (~{SAMPLE_FPS} fps); window={WINDOW_FRAMES}, step={step_frames}, clf_res={CLF_RES}\")\n",
        "\n",
        "# Per-track state\n",
        "buffers              = defaultdict(lambda: deque(maxlen=WINDOW_FRAMES))  # cow_id -> deque of padded crops\n",
        "last_sample_frame    = defaultdict(lambda: -10**9)                        # cow_id -> last raw frame idx sampled\n",
        "last_classify_frame  = defaultdict(lambda: -10**9)                        # cow_id -> last raw frame idx classified\n",
        "last_seen_frame      = dict()                                            # cow_id -> last frame seen\n",
        "pred_hist_single     = defaultdict(lambda: deque(maxlen=SMOOTH_K))       # cow_id -> last K labels\n",
        "\n",
        "active_event = dict()  # cow_id -> {\"label\": str, \"start_frame\": int}\n",
        "events       = []      # finalized segments\n",
        "\n",
        "# Annotated video writer\n",
        "writer = None\n",
        "if WRITE_ANNOTATED_MP4:\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    writer = cv2.VideoWriter(str(OUT_DIR / \"annotated.mp4\"), fourcc, video_fps, (video_w, video_h))\n",
        "\n",
        "start_time = time.time()\n",
        "frame_idx  = -1\n",
        "\n",
        "# ByteTrack stream\n",
        "gen = yolo.track(\n",
        "    source=VIDEO_PATH, stream=True, imgsz=YOLO_IMGSZ,\n",
        "    conf=YOLO_CONF, iou=YOLO_IOU, tracker=TRACKER_YAML,\n",
        "    device=0 if device==\"cuda\" else 'cpu', verbose=False, persist=True\n",
        ")\n",
        "\n",
        "# Ensure fast tracker dependency is present\n",
        "try:\n",
        "    import lap\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Debug dumps disabled by default\n",
        "DEBUG_DUMPS = 3\n",
        "_dbg_dumped = 0\n",
        "\n",
        "for res in tqdm(gen, desc=\"Processing\"):\n",
        "    frame_idx += 1\n",
        "    img_bgr = res.orig_img\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    xyxy = None; ids_arr = None\n",
        "    if res.boxes is not None and res.boxes.xyxy is not None:\n",
        "        xyxy = res.boxes.xyxy.cpu().numpy()\n",
        "        ids  = res.boxes.id\n",
        "        ids_arr = ids.cpu().numpy().astype(int) if ids is not None else np.arange(len(xyxy), dtype=int)\n",
        "\n",
        "        for bb, tid in zip(xyxy, ids_arr):\n",
        "            last_seen_frame[tid] = frame_idx\n",
        "\n",
        "            # sample into clip buffer at SAMPLE_FPS\n",
        "            if frame_idx - last_sample_frame[tid] >= sample_stride:\n",
        "                crop = center_pad_bbox_crop(img_rgb, bb, pad_target=PAD_TARGET_HW, out_size=CLF_RES)\n",
        "                if crop is not None:\n",
        "                    buffers[tid].append(crop)\n",
        "                    last_sample_frame[tid] = frame_idx\n",
        "\n",
        "            # Readiness, spaced by SAMPLES since last classify\n",
        "            n_samples = len(buffers[tid])\n",
        "            ready_full = (n_samples >= WINDOW_FRAMES)\n",
        "            ready_warm = (active_event.get(tid) is None) and (n_samples >= MIN_WARMUP_FRAMES)\n",
        "            step_ok    = (frame_idx - last_classify_frame[tid]) >= (sample_stride * step_frames)\n",
        "\n",
        "            if (ready_full or ready_warm) and step_ok:\n",
        "                clip = list(buffers[tid])\n",
        "                if n_samples < WINDOW_FRAMES:\n",
        "                    clip = clip + [clip[-1]] * (WINDOW_FRAMES - n_samples)  # pad to T\n",
        "\n",
        "                pixel_values = preprocess_clip(clip).to(device)\n",
        "                with torch.no_grad():\n",
        "                    logits   = clf_single(pixel_values=pixel_values).logits\n",
        "                    pred_idx = int(torch.argmax(logits, dim=-1).item())\n",
        "                    label    = SINGLE_ID2LABEL.get(pred_idx, str(pred_idx))\n",
        "                pred_hist_single[tid].append(label)\n",
        "                smooth_label = majority(list(pred_hist_single[tid]))\n",
        "                last_classify_frame[tid] = frame_idx\n",
        "\n",
        "                # Debug dump\n",
        "                if _dbg_dumped < DEBUG_DUMPS:\n",
        "                    T = len(clip); cols=4; rows=math.ceil(T/cols)\n",
        "                    padf = np.zeros_like(clip[0], dtype=np.uint8)\n",
        "                    tiles = clip + [padf]*(rows*cols - T)\n",
        "                    grid = np.concatenate([np.concatenate(tiles[r*cols:(r+1)*cols], 1) for r in range(rows)], 0)\n",
        "                    out_path = OUT_DIR / f\"dbg_cow{tid}_f{frame_idx}_raw-{label.replace(' ','_')}_sm-{smooth_label.replace(' ','_')}.jpg\"\n",
        "                    cv2.imwrite(str(out_path), cv2.cvtColor(grid, cv2.COLOR_RGB2BGR))\n",
        "                    _dbg_dumped += 1\n",
        "\n",
        "                # Segmenting around window center\n",
        "                window_center_frame = frame_idx - (WINDOW_FRAMES // 2)\n",
        "                if tid not in active_event:\n",
        "                    active_event[tid] = {\"label\": smooth_label, \"start_frame\": max(0, window_center_frame)}\n",
        "                else:\n",
        "                    if smooth_label != active_event[tid][\"label\"]:\n",
        "                        st = active_event[tid][\"start_frame\"]\n",
        "                        et = max(st, window_center_frame)\n",
        "                        if (et - st) / video_fps >= MIN_SEGMENT_S:\n",
        "                            events.append({\n",
        "                                \"cow_id\": int(tid),\n",
        "                                \"activity\": active_event[tid][\"label\"],\n",
        "                                \"start_frame\": st,\n",
        "                                \"end_frame\": et\n",
        "                            })\n",
        "                        active_event[tid] = {\"label\": smooth_label, \"start_frame\": window_center_frame}\n",
        "\n",
        "    # close stale tracks\n",
        "    to_close = []\n",
        "    for tid, lastf in list(last_seen_frame.items()):\n",
        "        if frame_idx - lastf >= timeout_frames:\n",
        "            to_close.append(tid)\n",
        "    for tid in to_close:\n",
        "        if tid in active_event:\n",
        "            st = active_event[tid][\"start_frame\"]\n",
        "            et = last_seen_frame[tid]\n",
        "            if et < st: et = st\n",
        "            if (et - st) / video_fps >= MIN_SEGMENT_S:\n",
        "                events.append({\n",
        "                    \"cow_id\": int(tid),\n",
        "                    \"activity\": active_event[tid][\"label\"],\n",
        "                    \"start_frame\": st,\n",
        "                    \"end_frame\": et\n",
        "                })\n",
        "        buffers.pop(tid, None)\n",
        "        pred_hist_single.pop(tid, None)\n",
        "        active_event.pop(tid, None)\n",
        "        last_seen_frame.pop(tid, None)\n",
        "\n",
        "    # overlay\n",
        "    if WRITE_ANNOTATED_MP4 and (xyxy is not None):\n",
        "        for bb, tid in zip(xyxy, ids_arr):\n",
        "            lab = active_event.get(int(tid), {}).get(\"label\")\n",
        "            if lab is None:\n",
        "                lab = \"estimating…\"\n",
        "            x1,y1,x2,y2 = map(int, bb)\n",
        "            cv2.rectangle(img_bgr, (x1,y1), (x2,y2), (0,255,0), 2)\n",
        "            cv2.putText(img_bgr, f\"{int(tid)}: {lab}\", (x1, max(0,y1-6)),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2, cv2.LINE_AA)\n",
        "        writer.write(img_bgr)\n",
        "\n",
        "# flush at end\n",
        "for tid, ev in list(active_event.items()):\n",
        "    st = ev[\"start_frame\"]\n",
        "    et = frame_idx\n",
        "    if et < st: et = st\n",
        "    if (et - st) / video_fps >= MIN_SEGMENT_S:\n",
        "        events.append({\n",
        "            \"cow_id\": int(tid),\n",
        "            \"activity\": ev[\"label\"],\n",
        "            \"start_frame\": st,\n",
        "            \"end_frame\": et\n",
        "        })\n",
        "if writer is not None:\n",
        "    writer.release()\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"Processed {frame_idx+1} frames in {elapsed:.1f}s \"\n",
        "      f\"(~{(frame_idx+1)/max(1.0,elapsed):.1f} FPS incl. det+track+class).\")\n",
        "if CLF_RES == 112:\n",
        "    print(\"Note: 112×112 mode chosen (faster, slight accuracy drop).\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iebPE_tDLEHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7277fc36-8258-4ce6-c92a-72fbf3e6a574"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps=30.00 frames=5399 size=(1920x1080)\n",
            "Sample every 19 frames (~1.6 fps); window=12, step=6, clf_res=224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 5399it [03:58, 22.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 5399 frames in 238.8s (~22.6 FPS incl. det+track+class).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logs (CSV) + summary\n",
        "\n",
        "def save_and_summarize(events, out_dir: Path, fps: float):\n",
        "    if not events:\n",
        "        print(\"No segments produced — check model paths / labels / thresholds.\")\n",
        "        return None, None\n",
        "\n",
        "    df = pd.DataFrame(events)\n",
        "    df[\"start_sec\"]   = df[\"start_frame\"] / fps\n",
        "    df[\"end_sec\"]     = df[\"end_frame\"] / fps\n",
        "    df[\"duration_s\"]  = df[\"end_sec\"] - df[\"start_sec\"]\n",
        "    df[\"start_hms\"]   = df[\"start_sec\"].map(sec_to_hms)\n",
        "    df[\"end_hms\"]     = df[\"end_sec\"].map(sec_to_hms)\n",
        "    df[\"duration_min\"]= df[\"duration_s\"] / 60.0\n",
        "\n",
        "    csv_events = out_dir / \"cow_activity_events.csv\"\n",
        "    df.sort_values([\"cow_id\",\"start_frame\"]).to_csv(csv_events, index=False)\n",
        "    print(f\"Saved event log: {csv_events}\")\n",
        "\n",
        "    agg = (df.groupby([\"cow_id\",\"activity\"])[\"duration_s\"].sum().reset_index())\n",
        "    agg[\"duration_min\"] = agg[\"duration_s\"] / 60.0\n",
        "    csv_totals = out_dir / \"cow_activity_totals.csv\"\n",
        "    agg.sort_values([\"cow_id\",\"duration_s\"], ascending=[True, False]).to_csv(csv_totals, index=False)\n",
        "    print(f\"Saved totals:    {csv_totals}\")\n",
        "\n",
        "    print(\"\\nSample summary:\")\n",
        "    view = (agg.sort_values([\"cow_id\",\"duration_s\"], ascending=[True, False])\n",
        "              .groupby(\"cow_id\").head(5))\n",
        "    for _, row in view.iterrows():\n",
        "        print(f\"cow {int(row['cow_id'])}: {row['activity']} for {row['duration_min']:.1f} min\")\n",
        "    return csv_events, csv_totals\n",
        "\n",
        "csv_events, csv_totals = save_and_summarize(events, OUT_DIR, video_fps)\n",
        "\n"
      ],
      "metadata": {
        "id": "wf-zRIhyL7EH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e7d7b6a-9fcc-4875-affd-92fe7ecb2854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved event log: /content/cow_logs1/cow_activity_events.csv\n",
            "Saved totals:    /content/cow_logs1/cow_activity_totals.csv\n",
            "\n",
            "Sample summary:\n",
            "cow 1: Lying for 2.9 min\n",
            "cow 2: Feeding & Standing for 2.6 min\n",
            "cow 2: Standing for 0.2 min\n",
            "cow 2: Ruminating & Standing for 0.1 min\n",
            "cow 3: Ruminating & Standing for 0.9 min\n",
            "cow 3: Drinking for 0.7 min\n",
            "cow 3: Standing for 0.7 min\n",
            "cow 3: Feeding & Standing for 0.4 min\n",
            "cow 3: Feeding & Lying for 0.2 min\n",
            "cow 4: Drinking for 1.0 min\n",
            "cow 4: Feeding & Standing for 0.3 min\n",
            "cow 4: Ruminating & Standing for 0.1 min\n",
            "cow 4: Standing for 0.1 min\n",
            "cow 5: Lying for 0.5 min\n",
            "cow 5: Ruminating & Lying for 0.2 min\n",
            "cow 6: Feeding & Standing for 1.5 min\n",
            "cow 6: Standing for 0.8 min\n",
            "cow 6: Lying for 0.6 min\n",
            "cow 7: Feeding & Standing for 2.5 min\n",
            "cow 7: Standing for 0.3 min\n",
            "cow 7: Ruminating & Lying for 0.2 min\n",
            "cow 8: Standing for 1.7 min\n",
            "cow 8: Drinking for 0.6 min\n",
            "cow 8: Feeding & Standing for 0.6 min\n",
            "cow 10: Lying for 0.4 min\n",
            "cow 10: Ruminating & Lying for 0.1 min\n",
            "cow 11: Drinking for 0.7 min\n",
            "cow 11: Feeding & Standing for 0.6 min\n",
            "cow 11: Ruminating & Standing for 0.1 min\n"
          ]
        }
      ]
    }
  ]
}